{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d03f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whisper': 0, 'swooping': 1, 'goodfornothing': 2, 'head': 3, 'lips': 4, 'casually': 5, 'some': 6, 'mood': 7, 'would': 8, 'strange': 9, 'rain': 10, 'real': 11, 'found': 12, 'saw': 13, 'peered': 14, 'normally': 15, 'neither': 16, 'such': 17, 'drummed': 18, 'boy': 19, 'well': 20, 'sleep': 21, 'shot': 22, 'tight': 23, 'er': 24, 'tabby': 25, 'none': 26, 'dressed': 27, 'busy': 28, 'tail': 29, 'black': 30, 'happening': 31, 'do': 32, 'office': 33, 'kiss': 34, 'clock': 35, 'passed': 36, 'someone': 37, 'smile': 38, 'mysterious': 39, 'yelled': 40, 'pulled': 41, 'gets': 42, 'happy': 43, 'by': 44, 'around': 45, 'blushed': 46, 'albus': 47, 'hoped': 48, 'fixed': 49, 'together': 50, 'tantrum': 51, 'wrestled': 52, 'overhead': 53, 'dundee': 54, 'everything': 55, 'nose': 56, 'wife': 57, 'distance': 58, 'shouted': 59, 'strangely': 60, 'unable': 61, 'something': 62, 'angrily': 63, 'dashed': 64, 'pointed': 65, 'flickered': 66, 'turned': 67, 'openmouthed': 68, 'perfectly': 69, 'heads': 70, 'one': 71, 'straight': 72, 'bit': 73, 'cloaks': 74, 'lots': 75, 'spoke': 76, 'enraged': 77, 'lily': 78, 'gently': 79, 'keeping': 80, 'sight': 81, 'seem': 82, 'reported': 83, 'stranger': 84, 'rather': 85, 'see': 86, 'second': 87, 'past': 88, 'heard': 89, 'cold': 90, 'nor': 91, 'high': 92, 'twitched': 93, 'watching': 94, 'tie': 95, 'dark': 96, 'sipped': 97, 'flicked': 98, 'face': 99, 'want': 100, 'maps': 101, 'fond': 102, 'up': 103, 'reason': 104, 'promised': 105, 'back': 106, 'its': 107, 'sunrise': 108, 'help': 109, 'approve': 110, 'sleeping': 111, 'purple': 112, 'broken': 113, 'tall': 114, 'diggle': 115, 'although': 116, 'clicked': 117, 'fact': 118, 'a': 119, 'wall': 120, 'ruffled': 121, 'stupid': 122, 'wet': 123, 'picked': 124, 'saying': 125, 'day': 126, 'flooded': 127, 'there': 128, 'still': 129, 'said': 130, 'flocks': 131, 'nerve': 132, 'plain': 133, 'owl': 134, 'house': 135, 'thought': 136, 'most': 137, 'different': 138, 'whisperers': 139, 'halfmoon': 140, 'knocked': 141, 'don': 142, 'should': 143, 'crept': 144, 'powers': 145, 'near': 146, 'shooting': 147, 'apart': 148, 'bakery': 149, 'thank': 150, 'putouter': 151, 'determined': 152, 'celebrating': 153, 'trying': 154, 'seized': 155, 'didn': 156, 'quite': 157, 'cleared': 158, 'amount': 159, 'think': 160, 'blue': 161, 'careful': 162, 'read': 163, 'thing': 164, 'bag': 165, 'disappeared': 166, 'direction': 167, 'you': 168, 'seems': 169, 'came': 170, 'which': 171, 'reached': 172, 'agree': 173, 'sharply': 174, 'front': 175, 'only': 176, 'everyone': 177, 'godric': 178, 'they': 179, 'close': 180, 'owls': 181, 'loudly': 182, 'i': 183, 'mixing': 184, 'them': 185, 'caught': 186, 'shocked': 187, 'pocket': 188, 'somebody': 189, 'seconds': 190, 'stars': 191, 'nervously': 192, 'stiffly': 193, 'trick': 194, 'swept': 195, 'sounding': 196, 'allowed': 197, 'if': 198, 'same': 199, 'disturb': 200, 'an': 201, 'asleep': 202, 'spot': 203, 'four': 204, 'goodbye': 205, 'james': 206, 'funny': 207, 'forgotten': 208, 'concentrate': 209, 'markings': 210, 'unblinkingly': 211, 'care': 212, 'even': 213, 'maybe': 214, 'sure': 215, 'with': 216, 'problems': 217, 'corner': 218, 'crooked': 219, 'blame': 220, 'edge': 221, 'suppose': 222, 'sweet': 223, 'pull': 224, 'director': 225, 'not': 226, 'older': 227, 'another': 228, 'starts': 229, 'lemon': 230, 'fancy': 231, 'petunia': 232, 'met': 233, 'cats': 234, 'pavement': 235, 'weren': 236, 'seemed': 237, 'spying': 238, 'bowed': 239, 'affect': 240, 'from': 241, 'm': 242, 'then': 243, 'experts': 244, 'gray': 245, 'sir': 246, 'howard': 247, 'wasn': 248, 'violet': 249, 'steering': 250, 'muggle': 251, 'was': 252, 'stared': 253, 'known': 254, 'asked': 255, 'huddle': 256, 'telephone': 257, 'knew': 258, 'floor': 259, 'walk': 260, 'o': 261, 'completely': 262, 'every': 263, 'phoning': 264, 'door': 265, 'blonde': 266, 'though': 267, 'hair': 268, 'middle': 269, 'while': 270, 'jam': 271, 'told': 272, 'never': 273, 'twelve': 274, 'thankful': 275, 'flatter': 276, 'also': 277, 'when': 278, 'jim': 279, 'but': 280, 'several': 281, 'crowd': 282, 'sparkling': 283, 'birds': 284, 'midnight': 285, 'whatever': 286, 'now': 287, 'point': 288, 'we': 289, 'long': 290, 'voice': 291, 'so': 292, 'smiling': 293, 've': 294, 'complete': 295, 'country': 296, 'where': 297, 'person': 298, 'bound': 299, 'drops': 300, 'heart': 301, 'night': 302, 'seen': 303, 'being': 304, 'britain': 305, 'tea': 306, 'showing': 307, 'swooped': 308, 'realized': 309, 'dudley': 310, 'downpour': 311, 'good': 312, 'our': 313, 'mixed': 314, 'through': 315, 'potters': 316, 'silly': 317, 'unusually': 318, 'kent': 319, 'really': 320, 'this': 321, 'awake': 322, 'until': 323, 'pecked': 324, 'carrying': 325, 'away': 326, 'catch': 327, 'how': 328, 'mind': 329, 'age': 330, 'sleepiness': 331, 'those': 332, 'home': 333, 'subject': 334, 'exactly': 335, 'briefcase': 336, 'me': 337, 'lucky': 338, 'almost': 339, 'yawned': 340, 'dare': 341, 'craning': 342, 'knows': 343, 'chortled': 344, 'hurried': 345, 'finished': 346, 'five': 347, 'amuse': 348, 'noticing': 349, 'piercing': 350, 'chair': 351, 'supposed': 352, 'cups': 353, 'word': 354, 'rattled': 355, 'sharp': 356, 'evening': 357, 'keep': 358, 'sensible': 359, 'room': 360, 'ted': 361, 'before': 362, 'stare': 363, 'dared': 364, 'stuff': 365, 'gone': 366, 'armchair': 367, 'judging': 368, 'report': 369, 'behaving': 370, 'oddly': 371, 'enough': 372, 'driven': 373, 'since': 374, 'promise': 375, 'grin': 376, 'voldemort': 377, 'tin': 378, 'probably': 379, 'here': 380, 'sister': 381, 'sped': 382, 'way': 383, 'in': 384, 'is': 385, 'did': 386, 'persuade': 387, 'passersby': 388, 'tonight': 389, 'drove': 390, 'ask': 391, 'careless': 392, 'looked': 393, 'expected': 394, 'happily': 395, 'later': 396, 'standing': 397, 'dialing': 398, 'related': 399, 'young': 400, 'as': 401, 'dursley': 402, 'come': 403, 'more': 404, 'dozen': 405, 'couple': 406, 'surely': 407, 'calls': 408, 'split': 409, 'wondered': 410, 'news': 411, 'pair': 412, 'severelooking': 413, 'bright': 414, 'mr': 415, 'today': 416, 'nonsense': 417, 'drifting': 418, 'anxious': 419, 'grunted': 420, 'common': 421, 'lay': 422, 'nothing': 423, 'believe': 424, 'must': 425, 'lamp': 426, 'bedroom': 427, 'coldly': 428, 'man': 429, 'twice': 430, 'downright': 431, 'youknowwho': 432, 'my': 433, 'time': 434, 'brick': 435, 'bed': 436, 'jerked': 437, 'shape': 438, 'excitedly': 439, 'right': 440, 'else': 441, 'struck': 442, 'haven': 443, 'nasty': 444, 'afternoon': 445, 'drawn': 446, 'sideways': 447, 'imagining': 448, 'everywhere': 449, 'what': 450, 'however': 451, 'impatiently': 452, 'calmly': 453, 'son': 454, 'discuss': 455, 'can': 456, 'he': 457, 'dead': 458, 'shuddered': 459, 'pretended': 460, 'anything': 461, 'minutes': 462, 're': 463, 'distinctly': 464, 'ninth': 465, 'mention': 466, 'buckled': 467, 'anyone': 468, 'feasts': 469, 'weirdos': 470, 'certainly': 471, 'except': 472, 'rumor': 473, 'had': 474, 'drills': 475, 'gossiped': 476, 'turning': 477, 'off': 478, 'horribly': 479, 'hardly': 480, 'harder': 481, 'lights': 482, 'youknow': 483, 'living': 484, 'tiny': 485, 'noble': 486, 'husband': 487, 'oh': 488, 'missed': 489, 'drive': 490, 'streets': 491, 's': 492, 'mumbled': 493, 'grunnings': 494, 'story': 495, 'square': 496, 'newscaster': 497, 'could': 498, 'privet': 499, 'name': 500, 'quickly': 501, 'soon': 502, 'learned': 503, 'small': 504, 'bear': 505, 'contrary': 506, 'pattern': 507, 'toward': 508, 'words': 509, 'and': 510, 'quiver': 511, 'child': 512, 'fear': 513, 'bonfire': 514, 'sinking': 515, 'beard': 516, 'proper': 517, 'cloudy': 518, 'who': 519, 'hugged': 520, 'screaming': 521, 'mrs': 522, 'kind': 523, 'upstairs': 524, 'irritably': 525, 'perhaps': 526, 'hollow': 527, 'too': 528, 'getups': 529, 'stroked': 530, 'confusing': 531, 'nighttime': 532, 'legs': 533, 'cereal': 534, 'get': 535, 'sightings': 536, 'air': 537, 'always': 538, 'bun': 539, 'unusual': 540, 'anywhere': 541, 'rejoice': 542, 'know': 543, 'harry': 544, 'ground': 545, 'viewers': 546, 'act': 547, 'rumors': 548, 'number': 549, 'stood': 550, 'swapping': 551, 'funnylooking': 552, 'undursleyish': 553, 'imagination': 554, 'of': 555, 'flinched': 556, 'statue': 557, 'whether': 558, 'muttered': 559, 'woman': 560, 'tried': 561, 'discover': 562, 'it': 563, 'beadyeyed': 564, 'be': 565, 'that': 566, 'buy': 567, 'sniffed': 568, 'liked': 569, 'years': 570, 'tyke': 571, 'building': 572, 'after': 573, 'very': 574, 'possible': 575, 'parties': 576, 'let': 577, '<unk>': 578, 'these': 579, 'tawny': 580, 'use': 581, 'arrived': 582, 'yesterday': 583, 'squeaky': 584, 'highheeled': 585, 'moment': 586, 'madam': 587, 'owlfree': 588, 'road': 589, 'old': 590, 'hundreds': 591, 'weatherman': 592, 'cigarette': 593, 'dear': 594, 'hadn': 595, 'town': 596, 'any': 597, 'lighter': 598, 'bunch': 599, 'threw': 600, 'find': 601, 'true': 602, 'order': 603, 'nice': 604, 'receiver': 605, 'watched': 606, 'him': 607, 'tell': 608, 'flying': 609, 'have': 610, 'new': 611, 'wouldn': 612, 'involved': 613, 'sat': 614, 'has': 615, 'early': 616, 'appeared': 617, 'left': 618, 'shoo': 619, 'last': 620, 'walls': 621, 'week': 622, 'folks': 623, 'isn': 624, 'mustache': 625, 'frightened': 626, 'earmuffs': 627, 'yes': 628, 'her': 629, 'changed': 630, 'normal': 631, 'slammed': 632, 'suddenly': 633, 'light': 634, 'celebrate': 635, 'big': 636, 'held': 637, 'will': 638, 'say': 639, 'stretch': 640, 'firm': 641, 'move': 642, 'mcguffin': 643, 'signs': 644, 'his': 645, 'able': 646, 'll': 647, 'been': 648, 'pomfrey': 649, 'narrowed': 650, 'whole': 651, 'few': 652, 'pinpricks': 653, 'fences': 654, 't': 655, 'window': 656, 'across': 657, 'weather': 658, 'spectacles': 659, 'at': 660, 'glasses': 661, 'morning': 662, 'much': 663, 'eight': 664, 'pursed': 665, 'group': 666, 'far': 667, 'first': 668, 'she': 669, 'noticed': 670, 'suggest': 671, 'things': 672, 'thinking': 673, 'going': 674, 'set': 675, 'greatest': 676, 'all': 677, 'decided': 678, 'looking': 679, 'shake': 680, 'slipped': 681, 'snapped': 682, 'wheel': 683, 'mcgonagall': 684, 'secretary': 685, 'staring': 686, 'opinion': 687, 'inside': 688, 'sit': 689, 'yourself': 690, 'belt': 691, 'worried': 692, 'least': 693, 'lose': 694, 'livingroom': 695, 'or': 696, 'boring': 697, 'uneasy': 698, 'silently': 699, 'notice': 700, 'bet': 701, 'street': 702, 'wanted': 703, 'daylight': 704, 'walked': 705, 'sense': 706, 'important': 707, 'boots': 708, 'throwing': 709, 'than': 710, 'made': 711, 'look': 712, 'yorkshire': 713, 'broad': 714, 'bathroom': 715, 'realize': 716, 'dursleys': 717, 'thin': 718, 'usual': 719, 'neighbors': 720, 'single': 721, 'finer': 722, 'fingers': 723, 'the': 724, 'on': 725, 'whispering': 726, 'map': 727, 'dedalus': 728, 'gazed': 729, 'lot': 730, 'large': 731, 'worrying': 732, 'having': 733, 'cat': 734, 'eyed': 735, 'car': 736, 'went': 737, 'nearest': 738, 'doughnut': 739, 'hunt': 740, 'again': 741, 'cloak': 742, 'half': 743, 'wearing': 744, 'nephew': 745, 'flutter': 746, 'gave': 747, 'wrong': 748, 'rummaging': 749, 'hard': 750, 'reading': 751, 'sign': 752, 'two': 753, 'better': 754, 'robes': 755, 'pop': 756, 'glance': 757, 'obviously': 758, 'hoping': 759, 'about': 760, 'stiff': 761, 'muggles': 762, 'tuck': 763, 'behind': 764, 'woke': 765, 'sky': 766, 'over': 767, 'silver': 768, 'because': 769, 'nation': 770, 'popped': 771, 'into': 772, 'collecting': 773, 'called': 774, 'throat': 775, 'put': 776, 'stern': 777, 'blinked': 778, 'showers': 779, 'admiring': 780, 'wide': 781, 'traffic': 782, 'couldn': 783, 'moved': 784, 'himself': 785, 'precious': 786, 'garden': 787, 'dumbledore': 788, 'improve': 789, 'frozen': 790, 'work': 791, 'neck': 792, 'mirror': 793, 'stopped': 794, 'fine': 795, 'emeraldgreen': 796, 'hummed': 797, 'got': 798, 'd': 799, 'birdwatchers': 800, 'out': 801, 'ever': 802, 'acting': 803, 'secret': 804, 'call': 805, 'won': 806, 'professor': 807, 'seeing': 808, 'us': 809, 'pressed': 810, 'clutching': 811, 'dull': 812, 'nearly': 813, 'outside': 814, 'upset': 815, 'drop': 816, 'unsticking': 817, 'sorry': 818, 'parking': 819, 'open': 820, 'lunchtime': 821, 'explain': 822, 'like': 823, 'fashion': 824, 'daughter': 825, 'backed': 826, 'are': 827, 'unwelcome': 828, 'tuesday': 829, 'spent': 830, 'angry': 831, 'answer': 832, 'next': 833, 'clothes': 834, 'lately': 835, 'peculiar': 836, 'waiting': 837, 'people': 838, 'gasped': 839, 'eyes': 840, 'their': 841, 'stumbled': 842, 'spotted': 843, 'to': 844, 'instead': 845, 'just': 846, 'dinner': 847, 'other': 848, 'end': 849, 'harvey': 850, 'little': 851, 'baker': 852, 'choosing': 853, 'potter': 854, 'times': 855, 'driveway': 856, 'cheek': 857, 'eleven': 858, 'no': 859, 'why': 860, 'sitting': 861, 'beefy': 862, 'your': 863, 'stunt': 864, 'comforting': 865, 'emerald': 866, 'darkness': 867, 'harold': 868, 'useful': 869, 'might': 870, 'for': 871, 'rooted': 872, 'both': 873, 'exasperated': 874, 'were': 875, 'chuckled': 876, 'place': 877, 'down': 878, 'behavior': 879, 'fell': 880, 'finally': 881}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "#Function to clean tokens\n",
    "def clean_token(token):\n",
    "    cleaned_token = re.sub(r'[^a-zA-Z0-9\\s]', '', token)\n",
    "    return cleaned_token\n",
    "\n",
    "with open('harrypotter.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "cleaned_tokens = [clean_token(token) for token in tokens]\n",
    "\n",
    "cleaned_tokens = [token for token in cleaned_tokens if token]\n",
    "\n",
    "vocab = set(cleaned_tokens)\n",
    "vocablist = list(vocab)\n",
    "\n",
    "vocab.add('<unk>')\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(word2idx)\n",
    "\n",
    "\n",
    "def getResponse(text, num_words=5):\n",
    "    random_words = random.sample(vocablist, num_words)\n",
    "    resp = ' '.join(random_words)\n",
    "    return resp\n",
    "\n",
    "\n",
    "\n",
    "def tokens_to_tensor(tokens):\n",
    "    indices = [word2idx.get(token, 1) for token in tokens] \n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "#chatBot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0dafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.5780\n",
      "Epoch 2/10, Loss: 5.8156\n",
      "Epoch 3/10, Loss: 5.4719\n",
      "Epoch 4/10, Loss: 5.2218\n",
      "Epoch 5/10, Loss: 5.1101\n",
      "Epoch 6/10, Loss: 4.9536\n",
      "Epoch 7/10, Loss: 4.8696\n",
      "Epoch 8/10, Loss: 4.7734\n",
      "Epoch 9/10, Loss: 4.7452\n",
      "Epoch 10/10, Loss: 4.6726\n",
      "Chatbot: Hi! I am your simple chatbot. Type 'quit' to end the chat.\n",
      "You: voldemort said\n",
      "seedtokens =  ['voldemort', 'said']\n",
      "input_tensor tensor([[377, 130]])\n",
      "Top 3 most probable words are:\n",
      "Word: the, Probability: 0.2793\n",
      "Word: dumbledore, Probability: 0.1248\n",
      "Word: a, Probability: 0.0542\n",
      "input_tensor tensor([[377, 130, 119]])\n",
      "Top 3 most probable words are:\n",
      "Word: piercing, Probability: 0.7809\n",
      "Word: woman, Probability: 0.1822\n",
      "Word: cat, Probability: 0.0080\n",
      "input_tensor tensor([[377, 130, 119, 560]])\n",
      "Top 3 most probable words are:\n",
      "Word: had, Probability: 0.9965\n",
      "Word: james, Probability: 0.0031\n",
      "Word: are, Probability: 0.0003\n",
      "input_tensor tensor([[377, 130, 119, 560, 827]])\n",
      "Top 3 most probable words are:\n",
      "Word: are, Probability: 0.9925\n",
      "Word: she, Probability: 0.0045\n",
      "Word: that, Probability: 0.0015\n",
      "input_tensor tensor([[377, 130, 119, 560, 827, 566]])\n",
      "Top 3 most probable words are:\n",
      "Word: they, Probability: 0.1368\n",
      "Word: lily, Probability: 0.0681\n",
      "Word: the, Probability: 0.0420\n",
      "input_tensor tensor([[377, 130, 119, 560, 827, 566, 179]])\n",
      "Top 3 most probable words are:\n",
      "Word: re, Probability: 0.9998\n",
      "Word: flatter, Probability: 0.0002\n",
      "Word: know, Probability: 0.0000\n",
      "input_tensor tensor([[377, 130, 119, 560, 827, 566, 179, 543]])\n",
      "Top 3 most probable words are:\n",
      "Word: piercing, Probability: 0.2567\n",
      "Word: as, Probability: 0.1223\n",
      "Word: drop, Probability: 0.1028\n",
      "input_tensor tensor([[377, 130, 119, 560, 827, 566, 179, 543, 816]])\n",
      "Top 3 most probable words are:\n",
      "Word: and, Probability: 0.9998\n",
      "Word: as, Probability: 0.0001\n",
      "Word: they, Probability: 0.0000\n",
      "Chatbot: voldemort said a woman are that they know drop as\n",
      "You: he knew\n",
      "seedtokens =  ['he', 'knew']\n",
      "input_tensor tensor([[457, 258]])\n",
      "Top 3 most probable words are:\n",
      "Word: that, Probability: 0.5414\n",
      "Word: voldemort, Probability: 0.1485\n",
      "Word: in, Probability: 0.1270\n",
      "input_tensor tensor([[457, 258, 377]])\n",
      "Top 3 most probable words are:\n",
      "Word: turned, Probability: 0.9992\n",
      "Word: madam, Probability: 0.0003\n",
      "Word: sideways, Probability: 0.0001\n",
      "input_tensor tensor([[457, 258, 377, 587]])\n",
      "Top 3 most probable words are:\n",
      "Word: pomfrey, Probability: 1.0000\n",
      "Word: answer, Probability: 0.0000\n",
      "Word: madam, Probability: 0.0000\n",
      "input_tensor tensor([[457, 258, 377, 587, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[457, 258, 377, 587, 832, 450]])\n",
      "Top 3 most probable words are:\n",
      "Word: she, Probability: 0.3674\n",
      "Word: dumbledore, Probability: 0.1342\n",
      "Word: they, Probability: 0.0820\n",
      "input_tensor tensor([[457, 258, 377, 587, 832, 450, 788]])\n",
      "Top 3 most probable words are:\n",
      "Word: bowed, Probability: 1.0000\n",
      "Word: answer, Probability: 0.0000\n",
      "Word: hard, Probability: 0.0000\n",
      "input_tensor tensor([[457, 258, 377, 587, 832, 450, 788, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[457, 258, 377, 587, 832, 450, 788, 832, 450]])\n",
      "Top 3 most probable words are:\n",
      "Word: she, Probability: 0.3674\n",
      "Word: dumbledore, Probability: 0.1342\n",
      "Word: they, Probability: 0.0820\n",
      "Chatbot: he knew voldemort madam answer what dumbledore answer what they\n",
      "You: they said\n",
      "seedtokens =  ['they', 'said']\n",
      "input_tensor tensor([[179, 130]])\n",
      "Top 3 most probable words are:\n",
      "Word: the, Probability: 0.2793\n",
      "Word: dumbledore, Probability: 0.1248\n",
      "Word: a, Probability: 0.0542\n",
      "input_tensor tensor([[179, 130, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[179, 130, 724, 473]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 1.0000\n",
      "Word: dead, Probability: 0.0000\n",
      "Word: night, Probability: 0.0000\n",
      "input_tensor tensor([[179, 130, 724, 473, 385]])\n",
      "Top 3 most probable words are:\n",
      "Word: that, Probability: 0.9961\n",
      "Word: up, Probability: 0.0019\n",
      "Word: answer, Probability: 0.0013\n",
      "input_tensor tensor([[179, 130, 724, 473, 385, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[179, 130, 724, 473, 385, 832, 450]])\n",
      "Top 3 most probable words are:\n",
      "Word: she, Probability: 0.3674\n",
      "Word: dumbledore, Probability: 0.1342\n",
      "Word: they, Probability: 0.0820\n",
      "input_tensor tensor([[179, 130, 724, 473, 385, 832, 450, 788]])\n",
      "Top 3 most probable words are:\n",
      "Word: bowed, Probability: 1.0000\n",
      "Word: answer, Probability: 0.0000\n",
      "Word: hard, Probability: 0.0000\n",
      "input_tensor tensor([[179, 130, 724, 473, 385, 832, 450, 788, 239]])\n",
      "Top 3 most probable words are:\n",
      "Word: his, Probability: 0.9995\n",
      "Word: he, Probability: 0.0001\n",
      "Word: the, Probability: 0.0001\n",
      "Chatbot: they said the rumor is answer what dumbledore bowed he\n",
      "You: potter died\n",
      "seedtokens =  ['potter', 'died']\n",
      "input_tensor tensor([[854, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[854, 724, 181]])\n",
      "Top 3 most probable words are:\n",
      "Word: are, Probability: 0.9514\n",
      "Word: night, Probability: 0.0345\n",
      "Word: a, Probability: 0.0019\n",
      "input_tensor tensor([[854, 724, 181, 827]])\n",
      "Top 3 most probable words are:\n",
      "Word: are, Probability: 0.9925\n",
      "Word: she, Probability: 0.0045\n",
      "Word: that, Probability: 0.0015\n",
      "input_tensor tensor([[854, 724, 181, 827, 669]])\n",
      "Top 3 most probable words are:\n",
      "Word: pressed, Probability: 0.9163\n",
      "Word: another, Probability: 0.0063\n",
      "Word: hard, Probability: 0.0045\n",
      "input_tensor tensor([[854, 724, 181, 827, 669, 750]])\n",
      "Top 3 most probable words are:\n",
      "Word: wall, Probability: 0.9997\n",
      "Word: dumbledore, Probability: 0.0002\n",
      "Word: on, Probability: 0.0000\n",
      "input_tensor tensor([[854, 724, 181, 827, 669, 750, 725]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 0.9997\n",
      "Word: a, Probability: 0.0001\n",
      "Word: and, Probability: 0.0000\n",
      "input_tensor tensor([[854, 724, 181, 827, 669, 750, 725, 385]])\n",
      "Top 3 most probable words are:\n",
      "Word: that, Probability: 0.9961\n",
      "Word: up, Probability: 0.0019\n",
      "Word: answer, Probability: 0.0013\n",
      "input_tensor tensor([[854, 724, 181, 827, 669, 750, 725, 385, 566]])\n",
      "Top 3 most probable words are:\n",
      "Word: they, Probability: 0.1368\n",
      "Word: lily, Probability: 0.0681\n",
      "Word: the, Probability: 0.0420\n",
      "Chatbot: potter the owls are she hard on is that the\n",
      "You: she said\n",
      "seedtokens =  ['she', 'said']\n",
      "input_tensor tensor([[669, 130]])\n",
      "Top 3 most probable words are:\n",
      "Word: the, Probability: 0.2793\n",
      "Word: dumbledore, Probability: 0.1248\n",
      "Word: a, Probability: 0.0542\n",
      "input_tensor tensor([[669, 130, 788]])\n",
      "Top 3 most probable words are:\n",
      "Word: bowed, Probability: 1.0000\n",
      "Word: answer, Probability: 0.0000\n",
      "Word: hard, Probability: 0.0000\n",
      "input_tensor tensor([[669, 130, 788, 239]])\n",
      "Top 3 most probable words are:\n",
      "Word: his, Probability: 0.9995\n",
      "Word: he, Probability: 0.0001\n",
      "Word: the, Probability: 0.0001\n",
      "input_tensor tensor([[669, 130, 788, 239, 457]])\n",
      "Top 3 most probable words are:\n",
      "Word: was, Probability: 0.1046\n",
      "Word: had, Probability: 0.0748\n",
      "Word: went, Probability: 0.0743\n",
      "input_tensor tensor([[669, 130, 788, 239, 457, 252]])\n",
      "Top 3 most probable words are:\n",
      "Word: choosing, Probability: 0.8122\n",
      "Word: true, Probability: 0.1397\n",
      "Word: dead, Probability: 0.0475\n",
      "input_tensor tensor([[669, 130, 788, 239, 457, 252, 602]])\n",
      "Top 3 most probable words are:\n",
      "Word: dumbledore, Probability: 0.9970\n",
      "Word: voldemort, Probability: 0.0021\n",
      "Word: professor, Probability: 0.0005\n",
      "input_tensor tensor([[669, 130, 788, 239, 457, 252, 602, 807]])\n",
      "Top 3 most probable words are:\n",
      "Word: mcgonagall, Probability: 0.9890\n",
      "Word: surely, Probability: 0.0038\n",
      "Word: i, Probability: 0.0007\n",
      "input_tensor tensor([[669, 130, 788, 239, 457, 252, 602, 807, 684]])\n",
      "Top 3 most probable words are:\n",
      "Word: gasped, Probability: 1.0000\n",
      "Word: dumbledore, Probability: 0.0000\n",
      "Word: she, Probability: 0.0000\n",
      "Chatbot: she said dumbledore bowed he was true professor mcgonagall gasped\n",
      "You: owls were\n",
      "seedtokens =  ['owls', 'were']\n",
      "input_tensor tensor([[181, 875]])\n",
      "Top 3 most probable words are:\n",
      "Word: bound, Probability: 0.8169\n",
      "Word: to, Probability: 0.0326\n",
      "Word: she, Probability: 0.0295\n",
      "input_tensor tensor([[181, 875, 669]])\n",
      "Top 3 most probable words are:\n",
      "Word: pressed, Probability: 0.9163\n",
      "Word: another, Probability: 0.0063\n",
      "Word: hard, Probability: 0.0045\n",
      "input_tensor tensor([[181, 875, 669, 750]])\n",
      "Top 3 most probable words are:\n",
      "Word: wall, Probability: 0.9997\n",
      "Word: dumbledore, Probability: 0.0002\n",
      "Word: on, Probability: 0.0000\n",
      "input_tensor tensor([[181, 875, 669, 750, 725]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 0.9997\n",
      "Word: a, Probability: 0.0001\n",
      "Word: and, Probability: 0.0000\n",
      "input_tensor tensor([[181, 875, 669, 750, 725, 510]])\n",
      "Top 3 most probable words are:\n",
      "Word: james, Probability: 0.9821\n",
      "Word: are, Probability: 0.0024\n",
      "Word: blonde, Probability: 0.0017\n",
      "input_tensor tensor([[181, 875, 669, 750, 725, 510, 206]])\n",
      "Top 3 most probable words are:\n",
      "Word: i, Probability: 0.9969\n",
      "Word: all, Probability: 0.0006\n",
      "Word: potter, Probability: 0.0004\n",
      "input_tensor tensor([[181, 875, 669, 750, 725, 510, 206, 183]])\n",
      "Top 3 most probable words are:\n",
      "Word: can, Probability: 0.4655\n",
      "Word: didn, Probability: 0.0229\n",
      "Word: she, Probability: 0.0190\n",
      "input_tensor tensor([[181, 875, 669, 750, 725, 510, 206, 183, 456]])\n",
      "Top 3 most probable words are:\n",
      "Word: t, Probability: 0.9994\n",
      "Word: answer, Probability: 0.0002\n",
      "Word: call, Probability: 0.0001\n",
      "Chatbot: owls were she hard on and james i can t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: they were\n",
      "seedtokens =  ['they', 'were']\n",
      "input_tensor tensor([[179, 875]])\n",
      "Top 3 most probable words are:\n",
      "Word: bound, Probability: 0.8169\n",
      "Word: to, Probability: 0.0326\n",
      "Word: she, Probability: 0.0295\n",
      "input_tensor tensor([[179, 875, 844]])\n",
      "Top 3 most probable words are:\n",
      "Word: find, Probability: 0.4800\n",
      "Word: believe, Probability: 0.1164\n",
      "Word: be, Probability: 0.0202\n",
      "input_tensor tensor([[179, 875, 844, 565]])\n",
      "Top 3 most probable words are:\n",
      "Word: dead, Probability: 0.8572\n",
      "Word: choosing, Probability: 0.0783\n",
      "Word: true, Probability: 0.0559\n",
      "input_tensor tensor([[179, 875, 844, 565, 458]])\n",
      "Top 3 most probable words are:\n",
      "Word: dumbledore, Probability: 0.9590\n",
      "Word: professor, Probability: 0.0388\n",
      "Word: james, Probability: 0.0010\n",
      "input_tensor tensor([[179, 875, 844, 565, 458, 807]])\n",
      "Top 3 most probable words are:\n",
      "Word: mcgonagall, Probability: 0.9890\n",
      "Word: surely, Probability: 0.0038\n",
      "Word: i, Probability: 0.0007\n",
      "input_tensor tensor([[179, 875, 844, 565, 458, 807, 183]])\n",
      "Top 3 most probable words are:\n",
      "Word: can, Probability: 0.4655\n",
      "Word: didn, Probability: 0.0229\n",
      "Word: she, Probability: 0.0190\n",
      "input_tensor tensor([[179, 875, 844, 565, 458, 807, 183, 156]])\n",
      "Top 3 most probable words are:\n",
      "Word: t, Probability: 0.9999\n",
      "Word: youknow, Probability: 0.0001\n",
      "Word: answer, Probability: 0.0000\n",
      "input_tensor tensor([[179, 875, 844, 565, 458, 807, 183, 156, 655]])\n",
      "Top 3 most probable words are:\n",
      "Word: believe, Probability: 0.4381\n",
      "Word: want, Probability: 0.0698\n",
      "Word: find, Probability: 0.0557\n",
      "Chatbot: they were to be dead professor i didn t want\n",
      "You: they scared\n",
      "seedtokens =  ['they', 'scared']\n",
      "input_tensor tensor([[179, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[179, 724, 316]])\n",
      "Top 3 most probable words are:\n",
      "Word: the, Probability: 0.9541\n",
      "Word: reason, Probability: 0.0053\n",
      "Word: he, Probability: 0.0036\n",
      "input_tensor tensor([[179, 724, 316, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[179, 724, 316, 724, 316]])\n",
      "Top 3 most probable words are:\n",
      "Word: the, Probability: 0.9541\n",
      "Word: reason, Probability: 0.0053\n",
      "Word: he, Probability: 0.0036\n",
      "input_tensor tensor([[179, 724, 316, 724, 316, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[179, 724, 316, 724, 316, 724, 473]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 1.0000\n",
      "Word: dead, Probability: 0.0000\n",
      "Word: night, Probability: 0.0000\n",
      "input_tensor tensor([[179, 724, 316, 724, 316, 724, 473, 302]])\n",
      "Top 3 most probable words are:\n",
      "Word: voldemort, Probability: 1.0000\n",
      "Word: about, Probability: 0.0000\n",
      "Word: had, Probability: 0.0000\n",
      "input_tensor tensor([[179, 724, 316, 724, 316, 724, 473, 302, 377]])\n",
      "Top 3 most probable words are:\n",
      "Word: turned, Probability: 0.9992\n",
      "Word: madam, Probability: 0.0003\n",
      "Word: sideways, Probability: 0.0001\n",
      "Chatbot: they the potters the potters the rumor night voldemort sideways\n",
      "You: the rumour\n",
      "seedtokens =  ['the', 'rumour']\n",
      "input_tensor tensor([[724, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[724, 724, 473]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 1.0000\n",
      "Word: dead, Probability: 0.0000\n",
      "Word: night, Probability: 0.0000\n",
      "input_tensor tensor([[724, 724, 473, 385]])\n",
      "Top 3 most probable words are:\n",
      "Word: that, Probability: 0.9961\n",
      "Word: up, Probability: 0.0019\n",
      "Word: answer, Probability: 0.0013\n",
      "input_tensor tensor([[724, 724, 473, 385, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[724, 724, 473, 385, 832, 486]])\n",
      "Top 3 most probable words are:\n",
      "Word: to, Probability: 0.9302\n",
      "Word: dumbledore, Probability: 0.0311\n",
      "Word: as, Probability: 0.0309\n",
      "input_tensor tensor([[724, 724, 473, 385, 832, 486, 788]])\n",
      "Top 3 most probable words are:\n",
      "Word: bowed, Probability: 1.0000\n",
      "Word: answer, Probability: 0.0000\n",
      "Word: hard, Probability: 0.0000\n",
      "input_tensor tensor([[724, 724, 473, 385, 832, 486, 788, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[724, 724, 473, 385, 832, 486, 788, 832, 486]])\n",
      "Top 3 most probable words are:\n",
      "Word: to, Probability: 0.9302\n",
      "Word: dumbledore, Probability: 0.0311\n",
      "Word: as, Probability: 0.0309\n",
      "Chatbot: the the rumor is answer noble dumbledore answer noble as\n",
      "You: the answer\n",
      "seedtokens =  ['the', 'answer']\n",
      "input_tensor tensor([[724, 832]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 1.0000\n",
      "Word: distinctly, Probability: 0.0000\n",
      "Word: noble, Probability: 0.0000\n",
      "input_tensor tensor([[724, 832, 450]])\n",
      "Top 3 most probable words are:\n",
      "Word: she, Probability: 0.3674\n",
      "Word: dumbledore, Probability: 0.1342\n",
      "Word: they, Probability: 0.0820\n",
      "input_tensor tensor([[724, 832, 450, 669]])\n",
      "Top 3 most probable words are:\n",
      "Word: pressed, Probability: 0.9163\n",
      "Word: another, Probability: 0.0063\n",
      "Word: hard, Probability: 0.0045\n",
      "input_tensor tensor([[724, 832, 450, 669, 750]])\n",
      "Top 3 most probable words are:\n",
      "Word: wall, Probability: 0.9997\n",
      "Word: dumbledore, Probability: 0.0002\n",
      "Word: on, Probability: 0.0000\n",
      "input_tensor tensor([[724, 832, 450, 669, 750, 725]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 0.9997\n",
      "Word: a, Probability: 0.0001\n",
      "Word: and, Probability: 0.0000\n",
      "input_tensor tensor([[724, 832, 450, 669, 750, 725, 119]])\n",
      "Top 3 most probable words are:\n",
      "Word: piercing, Probability: 0.7809\n",
      "Word: woman, Probability: 0.1822\n",
      "Word: cat, Probability: 0.0080\n",
      "input_tensor tensor([[724, 832, 450, 669, 750, 725, 119, 734]])\n",
      "Top 3 most probable words are:\n",
      "Word: nor, Probability: 0.5715\n",
      "Word: had, Probability: 0.0281\n",
      "Word: it, Probability: 0.0263\n",
      "input_tensor tensor([[724, 832, 450, 669, 750, 725, 119, 734, 563]])\n",
      "Top 3 most probable words are:\n",
      "Word: was, Probability: 0.0907\n",
      "Word: until, Probability: 0.0476\n",
      "Word: didn, Probability: 0.0401\n",
      "Chatbot: the answer what she hard on a cat it was\n",
      "You: it was\n",
      "seedtokens =  ['it', 'was']\n",
      "input_tensor tensor([[563, 252]])\n",
      "Top 3 most probable words are:\n",
      "Word: choosing, Probability: 0.8122\n",
      "Word: true, Probability: 0.1397\n",
      "Word: dead, Probability: 0.0475\n",
      "input_tensor tensor([[563, 252, 853]])\n",
      "Top 3 most probable words are:\n",
      "Word: another, Probability: 1.0000\n",
      "Word: arrived, Probability: 0.0000\n",
      "Word: sideways, Probability: 0.0000\n",
      "input_tensor tensor([[563, 252, 853, 447]])\n",
      "Top 3 most probable words are:\n",
      "Word: she, Probability: 0.5702\n",
      "Word: around, Probability: 0.1984\n",
      "Word: half, Probability: 0.1880\n",
      "input_tensor tensor([[563, 252, 853, 447,  45]])\n",
      "Top 3 most probable words are:\n",
      "Word: what, Probability: 0.9753\n",
      "Word: you, Probability: 0.0182\n",
      "Word: is, Probability: 0.0063\n",
      "input_tensor tensor([[563, 252, 853, 447,  45, 385]])\n",
      "Top 3 most probable words are:\n",
      "Word: that, Probability: 0.9961\n",
      "Word: up, Probability: 0.0019\n",
      "Word: answer, Probability: 0.0013\n",
      "input_tensor tensor([[563, 252, 853, 447,  45, 385, 103]])\n",
      "Top 3 most probable words are:\n",
      "Word: in, Probability: 0.9988\n",
      "Word: he, Probability: 0.0002\n",
      "Word: everyone, Probability: 0.0001\n",
      "input_tensor tensor([[563, 252, 853, 447,  45, 385, 103, 457]])\n",
      "Top 3 most probable words are:\n",
      "Word: was, Probability: 0.1046\n",
      "Word: had, Probability: 0.0748\n",
      "Word: went, Probability: 0.0743\n",
      "input_tensor tensor([[563, 252, 853, 447,  45, 385, 103, 457, 737]])\n",
      "Top 3 most probable words are:\n",
      "Word: to, Probability: 1.0000\n",
      "Word: the, Probability: 0.0000\n",
      "Word: any, Probability: 0.0000\n",
      "Chatbot: it was choosing sideways around is up he went any\n",
      "You: he went\n",
      "seedtokens =  ['he', 'went']\n",
      "input_tensor tensor([[457, 737]])\n",
      "Top 3 most probable words are:\n",
      "Word: to, Probability: 1.0000\n",
      "Word: the, Probability: 0.0000\n",
      "Word: any, Probability: 0.0000\n",
      "input_tensor tensor([[457, 737, 724]])\n",
      "Top 3 most probable words are:\n",
      "Word: rumor, Probability: 0.6236\n",
      "Word: potters, Probability: 0.2474\n",
      "Word: owls, Probability: 0.0071\n",
      "input_tensor tensor([[457, 737, 724, 473]])\n",
      "Top 3 most probable words are:\n",
      "Word: is, Probability: 1.0000\n",
      "Word: dead, Probability: 0.0000\n",
      "Word: night, Probability: 0.0000\n",
      "input_tensor tensor([[457, 737, 724, 473, 302]])\n",
      "Top 3 most probable words are:\n",
      "Word: voldemort, Probability: 1.0000\n",
      "Word: about, Probability: 0.0000\n",
      "Word: had, Probability: 0.0000\n",
      "input_tensor tensor([[457, 737, 724, 473, 302, 377]])\n",
      "Top 3 most probable words are:\n",
      "Word: turned, Probability: 0.9992\n",
      "Word: madam, Probability: 0.0003\n",
      "Word: sideways, Probability: 0.0001\n",
      "input_tensor tensor([[457, 737, 724, 473, 302, 377,  67]])\n",
      "Top 3 most probable words are:\n",
      "Word: up, Probability: 1.0000\n",
      "Word: feasts, Probability: 0.0000\n",
      "Word: phoning, Probability: 0.0000\n",
      "input_tensor tensor([[457, 737, 724, 473, 302, 377,  67, 103]])\n",
      "Top 3 most probable words are:\n",
      "Word: in, Probability: 0.9988\n",
      "Word: he, Probability: 0.0002\n",
      "Word: everyone, Probability: 0.0001\n",
      "input_tensor tensor([[457, 737, 724, 473, 302, 377,  67, 103, 384]])\n",
      "Top 3 most probable words are:\n",
      "Word: godric, Probability: 0.9999\n",
      "Word: rumor, Probability: 0.0000\n",
      "Word: his, Probability: 0.0000\n",
      "Chatbot: he went the rumor night voldemort turned up in rumor\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        self.values = nn.Linear(embed_size, embed_size)  \n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, values, keys, queries):\n",
    "        N = values.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        #print(\"Before: values = \",values)\n",
    "        \n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        \n",
    "#         print(\"After: values = \",values)\n",
    "#         print(\"After: keys = \",keys)\n",
    "#         print(\"After: queries = \",queries)\n",
    "        \n",
    "        \n",
    "        values = values.view(N, value_len, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        queries = queries.view(N, query_len, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attention_score = torch.einsum(\"nqhd,nkhd->nhqk\", queries, keys) / (self.head_dim ** 0.5)\n",
    "        #print(\"attention_score\", attention_score)\n",
    "        \n",
    "        attention = torch.softmax(attention_score, dim=-1)\n",
    "        #print(\"attention\", attention)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", attention, values).reshape(N, query_len, -1)\n",
    "        #print(\"out\", out)\n",
    "        \n",
    "        return self.fc_out(out)\n",
    "        \n",
    "        \n",
    "class SentenceGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, heads, max_len=10):\n",
    "        super(SentenceGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        #print(\"embedding = \",embedding)\n",
    "        attention = self.attention(embedding, embedding, embedding)\n",
    "        out = self.fc_out(attention)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def generate_sentence(self, user_input, temperature=1.0):\n",
    "        seed_tokens = word_tokenize(user_input.lower())\n",
    "        seed_idx = [word2idx.get(word, word2idx['the']) for word in seed_tokens]\n",
    "        sentence = seed_idx[:]\n",
    "        \n",
    "        print(\"seedtokens = \",seed_tokens)\n",
    "        #print(\"seedIdx = \",seed_idx)\n",
    "        \n",
    "        for _ in range(self.max_len - len(seed_idx)):\n",
    "            \n",
    "            input_tensor = torch.tensor(sentence, dtype= torch.long).unsqueeze(0)\n",
    "            print(\"input_tensor\", input_tensor)\n",
    "            logits = self.forward(input_tensor)\n",
    "            #print(\"logits\", logits)\n",
    "            logits = logits.squeeze(0)\n",
    "            #print(\"After sqeeze logits\", logits)\n",
    "            probs = torch.softmax(logits[-1], dim=-1)\n",
    "            #print(\"probs\", probs)\n",
    "            top_probs, top_idx = torch.topk(probs, 3)\n",
    "            #top_probs = top_probs.sqeeze().tolist()\n",
    "            top_idx = top_idx.squeeze().tolist()\n",
    "        \n",
    "            print(\"Top 3 most probable words are:\")\n",
    "            for idx, prob in zip(top_idx, top_probs):\n",
    "                word = idx2word.get(idx, '<unk>')\n",
    "                print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "        \n",
    "            next_word = random.choices(top_idx, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "        return ' '.join([idx2word.get(idx, 'the') for idx in sentence])\n",
    "    \n",
    "def trainings_pairs(tokens, seq_len=3):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(tokens) - seq_len):\n",
    "        input_seq = tokens[i:i+seq_len]\n",
    "        target_seq = tokens[i+1:i+1+seq_len]\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    input_tensors = [tokens_to_tensor(seq) for seq in inputs]\n",
    "    target_tensors = [tokens_to_tensor(seq) for seq in targets]\n",
    "    return input_tensors, target_tensors\n",
    "    \n",
    "def train(model, input_tensors, target_tensors, epochs, batch_size=1):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(input_tensors), batch_size):\n",
    "            batch_inputs = input_tensors[i:i+batch_size]\n",
    "            batch_targets = target_tensors[i:i+batch_size]\n",
    "            for inputs, targets in zip(batch_inputs, batch_targets):\n",
    "                inputs, targets = inputs.unsqueeze(0), targets.unsqueeze(0)\n",
    "                outputs = model(inputs).view(-1, len(vocab))\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(input_tensors)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "def chat_with_bot(model):\n",
    "    print(\"Chatbot: Hi! I am your simple chatbot. Type 'quit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Chatbot: Bye! Have a great day!\")\n",
    "            break\n",
    "        response = model.generate_sentence(user_input)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "embed_size = 256\n",
    "heads = 1\n",
    "model = SentenceGenerator(len(vocab), embed_size, heads)\n",
    "\n",
    "input_tensors, target_tensors = trainings_pairs(cleaned_tokens)\n",
    "train(model, input_tensors, target_tensors, epochs=10)\n",
    "\n",
    "chat_with_bot(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca0083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fab473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
